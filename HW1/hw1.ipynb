{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X_Te27fi-0pP"
   },
   "source": [
    "# **HW1: Regression** \n",
    "In *assignment 1*, you need to finish:\n",
    "\n",
    "1.  Basic Part: Implement the regression model to predict the number of dengue cases\n",
    "\n",
    "\n",
    "> *   Step 1: Split Data\n",
    "> *   Step 2: Preprocess Data\n",
    "> *   Step 3: Implement Regression\n",
    "> *   Step 4: Make Prediction\n",
    "> *   Step 5: Train Model and Generate Result\n",
    "\n",
    "2.  Advanced Part: Implementing a regression model to predict the number of dengue cases in a different way than the basic part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_wDdnos-4uUv"
   },
   "source": [
    "# 1. Basic Part (60%)\n",
    "In the first part, you need to implement the regression to predict the number of dengue cases\n",
    "\n",
    "Please save the prediction result in a csv file **hw1_basic.csv**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RzCR7vk9BFkf"
   },
   "source": [
    "## Import Packages\n",
    "\n",
    "> Note: You **cannot** import any other package in the basic part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "HL5XjqFf4wSj"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import csv\n",
    "import math\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jnWjrzi0dMPz"
   },
   "source": [
    "## Global attributes\n",
    "Define the global attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "EWLDPOlHBbcK"
   },
   "outputs": [],
   "source": [
    "input_dataroot = 'hw1_basic_input.csv' # Input file named as 'hw1_basic_input.csv'\n",
    "output_dataroot = 'hw1_basic.csv' # Output file will be named as 'hw1_basic.csv'\n",
    "\n",
    "input_datalist =  [] # Initial datalist, saved as numpy array\n",
    "output_datalist =  [] # Your prediction, should be 10 * 4 matrix and saved as numpy array\n",
    "             # The format of each row should be ['epiweek', 'CityA', 'CityB', 'CityC']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PsFC-cvqIcYK"
   },
   "source": [
    "You can add your own global attributes here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "OUbS2BEgcut6"
   },
   "outputs": [],
   "source": [
    "training_datasetA = np.zeros((94, 1))\n",
    "training_datasetB = np.zeros((94, 1))\n",
    "training_datasetC = np.zeros((94, 1))\n",
    "case_datasetA = np.zeros((94, 1))\n",
    "case_datasetB = np.zeros((94, 1))\n",
    "case_datasetC = np.zeros((94, 1))\n",
    "validation_datasetA = np.zeros((10, 1))\n",
    "validation_datasetB = np.zeros((10, 1))\n",
    "validation_datasetC = np.zeros((10, 1))\n",
    "case_validation_datasetA = np.zeros((10, 1))\n",
    "case_validation_datasetB = np.zeros((10, 1))\n",
    "case_validation_datasetC = np.zeros((10, 1))\n",
    "pred_datasetA = np.zeros((10,1))\n",
    "pred_datasetB = np.zeros((10,1))\n",
    "pred_datasetC = np.zeros((10,1))\n",
    "\n",
    "lag_pred_datasetA = np.zeros((10,1))\n",
    "lag_pred_datasetB = np.zeros((10,1))\n",
    "lag_pred_datasetC = np.zeros((10,1))\n",
    "\n",
    "lag_datasetA = np.zeros((94,1))\n",
    "lag_datasetB = np.zeros((94,1))\n",
    "lag_datasetC = np.zeros((94,1))\n",
    "\n",
    "lag_valid_dataA = np.zeros((10,1))\n",
    "lag_valid_dataB = np.zeros((10,1))\n",
    "lag_valid_dataC = np.zeros((10,1))\n",
    "W1 = [0,0,0]\n",
    "W2 = [0,0,0]\n",
    "W3 = [0,0,0]\n",
    "time = np.zeros((94, 1))\n",
    "validtime = np.zeros((10, 1))\n",
    "pred_time = np.zeros((10, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rUoRFoQjBW5S"
   },
   "source": [
    "## Load the Input File\n",
    "First, load the basic input file **hw1_basic_input.csv**\n",
    "\n",
    "Input data would be stored in *input_datalist*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "dekR1KnqBtI6"
   },
   "outputs": [],
   "source": [
    "# Read input csv to datalist\n",
    "with open(input_dataroot, newline='') as csvfile:\n",
    "  input_datalist = np.array(list(csv.reader(csvfile)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6kYPuikLCFx4"
   },
   "source": [
    "## Implement the Regression Model\n",
    "\n",
    "> Note: It is recommended to use the functions we defined, you can also define your own functions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jWwdx06JNEYs"
   },
   "source": [
    "### Step 1: Split Data\n",
    "Split data in *input_datalist* into training dataset and validation dataset \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "USDciENcB-5F"
   },
   "outputs": [],
   "source": [
    "def SplitData():\n",
    "  global training_datasetA, training_datasetB, training_datasetC, case_datasetA, case_datasetB, case_datasetC\n",
    "  global validation_datasetA, validation_datasetB, validation_datasetC \n",
    "  global lag_valid_dataA, lag_valid_dataB, lag_valid_dataC, lag_datasetA, lag_datasetB, lag_datasetC\n",
    "  global case_validation_datasetA, case_validation_datasetB, case_validation_datasetC,lag_valid_dataA,lag_valid_dataB,lag_valid_dataC\n",
    "  global pred_datasetA, pred_datasetB, pred_datasetC\n",
    "  global lag_pred_datasetA, lag_pred_datasetB, lag_pred_datasetC, time, valid_time, pred_time\n",
    "  \n",
    "  time = input_datalist[1:95, 0]\n",
    "  valid_time = input_datalist[85:95, 0]\n",
    "  pred_time = input_datalist[95:105, 0]\n",
    "\n",
    "  training_datasetA = input_datalist[1:95, 1]\n",
    "  training_datasetB = input_datalist[1:95, 2]\n",
    "  training_datasetC = input_datalist[1:95, 3]\n",
    "\n",
    "  \n",
    "    \n",
    "  case_datasetA = input_datalist[1:95, 4]\n",
    "  case_datasetB = input_datalist[1:95, 5]\n",
    "  case_datasetC = input_datalist[1:95, 6]\n",
    "\n",
    "  validation_datasetA = input_datalist[85:95, 1]\n",
    "  validation_datasetB = input_datalist[85:95, 2]\n",
    "  validation_datasetC = input_datalist[85:95, 3]\n",
    "  \n",
    "  lag_valid_dataA = input_datalist[84:94, 4]\n",
    "  lag_valid_dataB = input_datalist[84:94, 5]\n",
    "  lag_valid_dataC = input_datalist[84:94, 6]\n",
    "  \n",
    "  lag_datasetA = input_datalist[0:94, 4]\n",
    "  lag_datasetB = input_datalist[0:94, 5]\n",
    "  lag_datasetC = input_datalist[0:94, 6]\n",
    "  \n",
    "  case_validation_datasetA = input_datalist[85:95, 4]\n",
    "  case_validation_datasetB = input_datalist[85:95, 5]\n",
    "  case_validation_datasetC = input_datalist[85:95, 6]\n",
    "  \n",
    "  pred_datasetA = input_datalist[95:105, 1]\n",
    "  pred_datasetB = input_datalist[95:105, 2]\n",
    "  pred_datasetC = input_datalist[95:105, 3]\n",
    "\n",
    "  lag_pred_datasetA = input_datalist[94:104, 1]\n",
    "  lag_pred_datasetB = input_datalist[94:104, 2]\n",
    "  lag_pred_datasetC = input_datalist[94:104, 3]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u-3Qln4aNgVy"
   },
   "source": [
    "### Step 2: Preprocess Data\n",
    "Handle the unreasonable data\n",
    "> Hint: Outlier and missing data can be handled by removing the data or adding the values with the help of statistics  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "XXvW1n_5NkQ5"
   },
   "outputs": [],
   "source": [
    "def PreprocessData():\n",
    "  global training_datasetA, training_datasetB, training_datasetC, case_datasetA, case_datasetB, case_datasetC\n",
    "  global validation_datasetA, validation_datasetB, validation_datasetC \n",
    "  global case_validation_datasetA, case_validation_datasetB, case_validation_datasetC\n",
    "  global lag_datasetA, lag_datasetB, lag_datasetC, time\n",
    "  A = []\n",
    "  B = []\n",
    "  C = []\n",
    "  D = []\n",
    "  for i in range(94):\n",
    "    if((training_datasetA[i] != '') and (float(training_datasetA[i]) < 40.0 and float(training_datasetA[i]) > 10.0)):\n",
    "      A.append(training_datasetA[i])\n",
    "      B.append(case_datasetA[i])\n",
    "      C.append(time[i])\n",
    "      D.append(lag_datasetA[i])\n",
    "  training_datasetA = A.copy()\n",
    "  case_datasetA = B.copy()\n",
    "  lag_datasetA = D.copy()\n",
    "  time = C.copy()\n",
    "  A = []\n",
    "  B = []\n",
    "  D = []\n",
    "  for i in range(94):\n",
    "    if((training_datasetB[i] != '') and (float(training_datasetB[i]) < 40.0 and float(training_datasetB[i]) > 10.0) ):\n",
    "      A.append(training_datasetB[i])\n",
    "      B.append(case_datasetB[i])\n",
    "      D.append(lag_datasetB[i])\n",
    "  training_datasetB = A.copy()\n",
    "  case_datasetB = B.copy()\n",
    "  lag_datasetB = D.copy()\n",
    "  A = []\n",
    "  B = []\n",
    "  D = []\n",
    "  for i in range(94):\n",
    "    if((training_datasetC[i] != '') and (float(training_datasetC[i]) < 40.0 and float(training_datasetC[i]) > 10.0) ):\n",
    "      A.append(training_datasetC[i])\n",
    "      B.append(case_datasetC[i])\n",
    "      D.append(lag_datasetC[i])\n",
    "  training_datasetC = A.copy()\n",
    "  case_datasetC = B.copy()\n",
    "  lag_datasetC = D.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yDLpJmQUN3V6"
   },
   "source": [
    "### Step 3: Implement Regression\n",
    "> Hint: You can use Matrix Inversion, or Gradient Descent to finish this part\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "Tx9n1_23N8C0"
   },
   "outputs": [],
   "source": [
    "def Regression():\n",
    "  global  lag_datasetA, lag_datasetB, lag_datasetC, W1, W2, W3\n",
    "  lenA = len(training_datasetA) \n",
    "  lenB = len(training_datasetB) \n",
    "  lenC = len(training_datasetC) \n",
    "  fi = np.full((lenA,3),0.0)\n",
    "  \n",
    "  lag_datasetA[0] = 0\n",
    "\n",
    "  y1 = np.full((lenA,1),0.0)\n",
    "  for i in range(1,lenA):\n",
    "    if(i >= 1):\n",
    "      fi[i-1][0] = float(lag_datasetA[i])\n",
    "      fi[i-1][1] = float(training_datasetA[i])\n",
    "      fi[i-1][2] = 1\n",
    "      \n",
    "      y1[i-1] = case_datasetA[i]\n",
    "  fi_t = fi.transpose()\n",
    "  W = np.matmul(fi_t, fi)\n",
    "  \n",
    "  T = np.linalg.inv(W)\n",
    "  W = np.matmul(T, fi_t)\n",
    "  W1 = np.matmul(W, y1)\n",
    "  #B\n",
    "  fi = np.full((lenB,3),0.0)\n",
    " \n",
    "  lag_datasetB[0] = 0\n",
    "  y2 = np.full((lenB,1),0.0)\n",
    "  for i in range(1,lenB):\n",
    "    if(i >= 1):\n",
    "      fi[i-1][0] = float(lag_datasetB[i])\n",
    "      fi[i-1][1] = float(training_datasetB[i])\n",
    "      fi[i-1][2] = 1\n",
    "      y2[i-1] = case_datasetB[i]\n",
    "  fi_t = fi.transpose()\n",
    "  \n",
    "  W = np.matmul(fi_t, fi)\n",
    "  T = np.linalg.inv(W)\n",
    "  W = np.matmul(T, fi_t)\n",
    "  W2 = np.matmul(W, y2)\n",
    "\n",
    "  #C\n",
    "  fi = np.full((lenC,3),0.0)\n",
    "  \n",
    "  lag_datasetC[0] = 0\n",
    "  y3 = np.full((lenC,1),0.0)\n",
    "  for i in range(1,lenC):\n",
    "    if(i >= 1):\n",
    "      fi[i-1][0] = float(lag_datasetC[i])\n",
    "      fi[i-1][1] = float(training_datasetC[i])\n",
    "      fi[i-1][2] = 1\n",
    "      y3[i-1] = case_datasetC[i]\n",
    "  fi_t = fi.transpose()\n",
    "  W = np.matmul(fi_t, fi)\n",
    "  T = np.linalg.inv(W)\n",
    "  W = np.matmul(T, fi_t)\n",
    "  W3 = np.matmul(W, y3)\n",
    "  \n",
    "  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2NxRNFwyN8xd"
   },
   "source": [
    "### Step 4: Make Prediction\n",
    "Make prediction of testing dataset and store the value in *output_datalist*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "EKlDIC2-N_lk"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29 [35.31462054]\n",
      "28 [31.7773839]\n",
      "31 [28.48126544]\n",
      "33 [31.50816718]\n",
      "23 [33.39825938]\n",
      "27 [25.22972388]\n",
      "27 [30.14472756]\n",
      "22 [29.38292758]\n",
      "25 [23.78294054]\n",
      "25 [26.88864918]\n",
      "MapeA : 15.730600311182569 %\n",
      "MapeB : 24.344892852032043 %\n",
      "MapeC : 14.254653128270917 %\n"
     ]
    }
   ],
   "source": [
    "def MakePrediction():\n",
    "  sum1 = 0\n",
    "  sum2 = 0\n",
    "  sum3 = 0\n",
    "  ans1 = []\n",
    "  ans2 = []\n",
    "  ans3 = []\n",
    "  tmp = []\n",
    "  for i in range(10):\n",
    "    ans1.append(W1[0] * float(lag_valid_dataA[i]) + W1[1] * float(validation_datasetA[i]) + W1[2])\n",
    "    ans2.append(W2[0] * float(lag_valid_dataB[i]) + W2[1] * float(validation_datasetB[i]) + W2[2])\n",
    "    ans3.append(W3[0] * float(lag_valid_dataC[i]) + W3[1] * float(validation_datasetC[i]) + W3[2])\n",
    "    \n",
    "    sum1 += abs((float(case_validation_datasetA[i]) - ans1[i])) / (float(case_validation_datasetA[i]))\n",
    "    sum2 += abs((float(case_validation_datasetB[i]) - ans2[i])) / (float(case_validation_datasetB[i]))\n",
    "    sum3 += abs((float(case_validation_datasetC[i]) - ans3[i])) / (float(case_validation_datasetC[i]))\n",
    "    print(case_validation_datasetA[i] ,ans1[i])\n",
    "  sum1 /= 10\n",
    "  sum2 /= 10\n",
    "  sum3 /= 10\n",
    "  print(\"MapeA :\" ,sum1[0] * 100, \"%\")\n",
    "  print(\"MapeB :\" ,sum2[0] * 100, \"%\")\n",
    "  print(\"MapeC :\" ,sum3[0] * 100, \"%\")\n",
    "  for i in range(10):\n",
    "    tmp = []\n",
    "    tmp.append(202143 + i)\n",
    "    tmp.append(np.round(W1[0] * float(lag_pred_datasetA[i]) + W1[1] * float(pred_datasetA[i]) + W1[2])[0])\n",
    "    tmp.append(np.round(W2[0] * float(lag_pred_datasetB[i]) + W2[1] * float(pred_datasetB[i]) + W2[2])[0])\n",
    "    tmp.append(np.round(W3[0] * float(lag_pred_datasetC[i]) + W3[1] * float(pred_datasetC[i]) + W3[2])[0])\n",
    "    output_datalist.append(tmp)\n",
    "    \n",
    "SplitData()\n",
    "PreprocessData()\n",
    "Regression()\n",
    "MakePrediction()\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cCd0Z6izOCwq"
   },
   "source": [
    "### Step 5: Train Model and Generate Result\n",
    "\n",
    "> Notice: **Remember to output the coefficients of the model here**, otherwise 5 points would be deducted\n",
    "* If your regression model is *3x^2 + 2x^1 + 1*, your output would be: \n",
    "```\n",
    "3 2 1\n",
    "```\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "iCL92EPKOFIn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8688661009053639 y(t-1) +  -0.5253793001816984 x +  19.325968799348384 \n",
      "\n",
      "0.7399599558334383 y(t-1) +  -0.8177508727289325 x +  26.067715484685714 \n",
      "\n",
      "0.9249896679921573 y(t-1) +  0.07517808191761745 x +  0.00024262950962050667 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(W1[0][0],\"y(t-1) + \",W1[1][0],\"x + \",W1[2][0],\"\\n\")\n",
    "print(W2[0][0],\"y(t-1) + \",W2[1][0],\"x + \",W2[2][0],\"\\n\")\n",
    "print(W3[0][0],\"y(t-1) + \",W3[1][0],\"x + \",W3[2][0],\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J8Jhd8wAOk3D"
   },
   "source": [
    "## Write the Output File\n",
    "Write the prediction to output csv\n",
    "> Format: 'epiweek', 'CityA', 'CityB', 'CityC'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "tYQVYLlKOtDB"
   },
   "outputs": [],
   "source": [
    "with open(output_dataroot, 'w', newline='', encoding=\"utf-8\") as csvfile:\n",
    "  writer = csv.writer(csvfile)\n",
    "  for row in output_datalist:\n",
    "    writer.writerow(row)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rx4408qg4xMQ"
   },
   "source": [
    "# 2. Advanced Part (35%)\n",
    "In the second part, you need to implement the regression in a different way than the basic part to help your predictions for the number of dengue cases\n",
    "\n",
    "We provide you with two files **hw1_advanced_input1.csv** and **hw1_advanced_input2.csv** that can help you in this part\n",
    "\n",
    "Please save the prediction result in a csv file **hw1_advanced.csv** \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "DaZCe19m41g1",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MapeA : [13.47840753] %\n",
      "MapeB : [23.03251359] %\n",
      "MapeC : [14.95886245] %\n"
     ]
    }
   ],
   "source": [
    "input_dataroot = 'hw1_basic_input.csv' # Input file named as 'hw1_basic_input.csv'\n",
    "output_dataroot = 'hw1_advanced.csv' # Output file will be named as 'hw1_basic.csv'\n",
    "\n",
    "input_datalist =  [] # Initial datalist, saved as numpy array\n",
    "output_datalist =  [] # Your prediction, should be 10 * 4 matrix and saved as numpy array\n",
    "             # The format of each row should be ['epiweek', 'CityA', 'CityB', 'CityC']\n",
    "\n",
    "training_datasetA = np.zeros((94, 1))\n",
    "training_datasetB = np.zeros((94, 1))\n",
    "training_datasetC = np.zeros((94, 1))\n",
    "case_datasetA = np.zeros((94, 1))\n",
    "case_datasetB = np.zeros((94, 1))\n",
    "case_datasetC = np.zeros((94, 1))\n",
    "validation_datasetA = np.zeros((10, 1))\n",
    "validation_datasetB = np.zeros((10, 1))\n",
    "validation_datasetC = np.zeros((10, 1))\n",
    "case_validation_datasetA = np.zeros((10, 1))\n",
    "case_validation_datasetB = np.zeros((10, 1))\n",
    "case_validation_datasetC = np.zeros((10, 1))\n",
    "\n",
    "lag2_datasetA = np.zeros((94,1))\n",
    "lag2_datasetB = np.zeros((94,1))\n",
    "lag2_datasetC = np.zeros((94,1))\n",
    "\n",
    "pred_datasetA = np.zeros((10,1))\n",
    "pred_datasetB = np.zeros((10,1))\n",
    "pred_datasetC = np.zeros((10,1))\n",
    "\n",
    "lag_pred_datasetA = np.zeros((10,1))\n",
    "lag_pred_datasetB = np.zeros((10,1))\n",
    "lag_pred_datasetC = np.zeros((10,1))\n",
    "\n",
    "lag_datasetA = np.zeros((94,1))\n",
    "lag_datasetB = np.zeros((94,1))\n",
    "lag_datasetC = np.zeros((94,1))\n",
    "\n",
    "lag_valid_dataA = np.zeros((10,1))\n",
    "lag_valid_dataB = np.zeros((10,1))\n",
    "lag_valid_dataC = np.zeros((10,1))\n",
    "W1 = [0,0,0]\n",
    "W2 = [0,0,0]\n",
    "W3 = [0,0,0]\n",
    "time = np.zeros((94, 1))\n",
    "validtime = np.zeros((10, 1))\n",
    "pred_time = np.zeros((10, 1))\n",
    "\n",
    "# Read input csv to datalist\n",
    "with open(input_dataroot, newline='') as csvfile:\n",
    "  input_datalist = np.array(list(csv.reader(csvfile)))\n",
    "\n",
    "def Advanced_PreprocessData():\n",
    "  global training_datasetA, training_datasetB, training_datasetC, case_datasetA, case_datasetB, case_datasetC\n",
    "  global validation_datasetA, validation_datasetB, validation_datasetC \n",
    "  global case_validation_datasetA, case_validation_datasetB, case_validation_datasetC\n",
    "  global time, valid_time, pred_time, lag_datasetA, lag_datasetB, lag_datasetC\n",
    "  A = []\n",
    "  B = []\n",
    "  c = []\n",
    "  D = []\n",
    "  for i in range(94):\n",
    "    if((training_datasetA[i] != '') and (float(training_datasetA[i]) < 40.0 and float(training_datasetA[i]) > 10.0)):\n",
    "      A.append(training_datasetA[i])\n",
    "      B.append(case_datasetA[i])\n",
    "      c.append(time[i])\n",
    "      D.append(lag_datasetA[i])\n",
    "  training_datasetA = A.copy()\n",
    "  case_datasetA = B.copy()\n",
    "  time = c.copy()\n",
    "  lag_datasetA = D.copy()\n",
    "  A = []\n",
    "  B = []\n",
    "  D = []\n",
    "  for i in range(94):\n",
    "    if((training_datasetB[i] != '') and (float(training_datasetB[i]) < 40.0 and float(training_datasetB[i]) > 10.0) ):\n",
    "      A.append(training_datasetB[i])\n",
    "      B.append(case_datasetB[i])\n",
    "      D.append(lag_datasetB[i])\n",
    "  training_datasetB = A.copy()\n",
    "  case_datasetB = B.copy()\n",
    "  lag_datasetB = D.copy()\n",
    "  A = []\n",
    "  B = []\n",
    "  D = []\n",
    "  for i in range(94):\n",
    "    if((training_datasetC[i] != '') and (float(training_datasetC[i]) < 40.0 and float(training_datasetC[i]) > 10.0) ):\n",
    "      A.append(training_datasetC[i])\n",
    "      B.append(case_datasetC[i])\n",
    "      D.append(lag_datasetC[i])\n",
    "  training_datasetC = A.copy()\n",
    "  case_datasetC = B.copy()\n",
    "  lag_datasetC = D.copy()\n",
    "\n",
    "def Advanced_Regression():\n",
    "  global  lag_datasetA, lag_datasetB, lag_datasetC, W1, W2, W3\n",
    "  lenA = len(training_datasetA) \n",
    "  lenB = len(training_datasetB) \n",
    "  lenC = len(training_datasetC) \n",
    "  ad_fi = np.full((lenA,4),0.0)\n",
    "  \n",
    "  lag_datasetA[0] = 0\n",
    "\n",
    "  y1 = np.full((lenA,1),0.0)\n",
    "  for i in range(1,lenA):\n",
    "    if(i >= 1):\n",
    "      ad_fi[i-1][0] = float(lag_datasetA[i])\n",
    "      ad_fi[i-1][1] = float(training_datasetA[i])\n",
    "      ad_fi[i-1][2] = float(time[i])\n",
    "      ad_fi[i-1][3] = 1\n",
    "      \n",
    "      y1[i-1] = case_datasetA[i]\n",
    "  \n",
    "  fi_t = ad_fi.transpose()\n",
    "  W = np.matmul(fi_t, ad_fi)\n",
    "  \n",
    "  T = np.linalg.inv(W)\n",
    "  W = np.matmul(T, fi_t)\n",
    "  W1 = np.matmul(W, y1)\n",
    "  \n",
    "  #B\n",
    "  ad_fi = np.full((lenB,4),0.0)\n",
    " \n",
    "  lag_datasetB[0] = 0\n",
    "  y2 = np.full((lenB,1),0.0)\n",
    "  for i in range(1,lenB):\n",
    "    if(i >= 1):\n",
    "      ad_fi[i-1][0] = float(lag_datasetB[i])\n",
    "      ad_fi[i-1][1] = float(training_datasetB[i])\n",
    "      ad_fi[i-1][2] = float(time[i])\n",
    "      ad_fi[i-1][3] = 1\n",
    "      y2[i-1] = case_datasetB[i]\n",
    "  fi_t = ad_fi.transpose()\n",
    "  \n",
    "  W = np.matmul(fi_t, ad_fi)\n",
    "  T = np.linalg.inv(W)\n",
    "  W = np.matmul(T, fi_t)\n",
    "  W2 = np.matmul(W, y2)\n",
    "  \n",
    "  #C\n",
    "  ad_fi = np.full((lenC,4),0.0)\n",
    "  \n",
    "  lag_datasetC[0] = 0\n",
    "  y3 = np.full((lenC,1),0.0)\n",
    "  for i in range(1,lenC):\n",
    "    if(i >= 1):\n",
    "      ad_fi[i-1][0] = float(lag_datasetC[i])\n",
    "      ad_fi[i-1][1] = float(training_datasetC[i])\n",
    "      ad_fi[i-1][2] = float(time[i])\n",
    "      ad_fi[i-1][3] = 1\n",
    "      y3[i-1] = case_datasetC[i]\n",
    "\n",
    "  fi_t = ad_fi.transpose()\n",
    "  W = np.matmul(fi_t, ad_fi)\n",
    "  T = np.linalg.inv(W)\n",
    "  W = np.matmul(T, fi_t)\n",
    "  W3 = np.matmul(W, y3)\n",
    "  \n",
    "def Advanced_MakePrediction():\n",
    "  sum1 = 0\n",
    "  sum2 = 0\n",
    "  sum3 = 0\n",
    "  ans1 = []\n",
    "  ans2 = []\n",
    "  ans3 = []\n",
    "  tmp = []\n",
    "  for i in range(10):\n",
    "    ans1.append(W1[0] * float(lag_valid_dataA[i]) + W1[1] * float(validation_datasetA[i]) + W1[2] * float(valid_time[i]) + W1[3])\n",
    "    ans2.append(W2[0] * float(lag_valid_dataB[i]) + W2[1] * float(validation_datasetB[i]) + W2[2] * float(valid_time[i]) + W2[3])\n",
    "    ans3.append(W3[0] * float(lag_valid_dataC[i]) + W3[1] * float(validation_datasetC[i]) + W3[2] * float(valid_time[i]) + W3[3])\n",
    "    \n",
    "    sum1 += abs((float(case_validation_datasetA[i]) - ans1[i])) / (float(case_validation_datasetA[i]))\n",
    "    sum2 += abs((float(case_validation_datasetB[i]) - ans2[i])) / (float(case_validation_datasetB[i]))\n",
    "    sum3 += abs((float(case_validation_datasetC[i]) - ans3[i])) / (float(case_validation_datasetC[i]))\n",
    "  sum1 /= 10\n",
    "  sum2 /= 10\n",
    "  sum3 /= 10\n",
    "  print(\"MapeA :\" ,sum1 * 100, \"%\")\n",
    "  print(\"MapeB :\" ,sum2 * 100, \"%\")\n",
    "  print(\"MapeC :\" ,sum3 * 100, \"%\")\n",
    "  for i in range(10):\n",
    "    tmp = []\n",
    "    tmp.append(202143 + i)\n",
    "    tmp.append(np.round(W1[0] * float(lag_pred_datasetA[i]) + W1[1] * float(pred_datasetA[i]) + W1[2] * float(pred_time[i]) + W1[3])[0])\n",
    "    tmp.append(np.round(W2[0] * float(lag_pred_datasetB[i]) + W2[1] * float(pred_datasetB[i]) + W2[2] * float(pred_time[i]) + W2[3])[0])\n",
    "    tmp.append(np.round(W3[0] * float(lag_pred_datasetC[i]) + W3[1] * float(pred_datasetC[i]) + W3[2] * float(pred_time[i]) + W3[3])[0])\n",
    "    output_datalist.append(tmp)\n",
    "    \n",
    "SplitData()\n",
    "Advanced_PreprocessData()\n",
    "Advanced_Regression()\n",
    "Advanced_MakePrediction()\n",
    "with open(output_dataroot, 'w', newline='', encoding=\"utf-8\") as csvfile:\n",
    "  writer = csv.writer(csvfile)\n",
    "  for row in output_datalist:\n",
    "    writer.writerow(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EtgCJU7FPeJL"
   },
   "source": [
    "# Report *(5%)*\n",
    "\n",
    "Report should be submitted as a pdf file **hw1_report.pdf**\n",
    "\n",
    "*   Briefly describe the difficulty you encountered \n",
    "*   Summarize your work and your reflections \n",
    "*   No more than one page\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hlEE53_MPf4W"
   },
   "source": [
    "# Save the Code File\n",
    "Please save your code and submit it as an ipynb file! (**hw1.ipynb**)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
